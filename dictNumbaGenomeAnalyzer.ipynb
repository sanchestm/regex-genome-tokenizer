{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2c5f1e5b-59a6-4aa1-b6d1-0ca27b130d3e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\thiag\\anaconda3\\envs\\UMAP\\lib\\site-packages\\umap_learn-0.5.0rc1-py3.8.egg\\umap\\__init__.py:9: UserWarning: Tensorflow not installed; ParametricUMAP will be unavailable\n",
      "  warn(\"Tensorflow not installed; ParametricUMAP will be unavailable\")\n"
     ]
    }
   ],
   "source": [
    "import dask.array as da\n",
    "import dask.bag as db\n",
    "from dask.dataframe import Series\n",
    "import dask.dataframe as dd\n",
    "import warnings\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from dask.distributed import Client, TimeoutError\n",
    "import scipy as sp\n",
    "import scipy.sparse as sps\n",
    "from numba.typed import Dict, List\n",
    "from numba import prange\n",
    "from numba.types import unicode_type\n",
    "from scipy.sparse import csr_matrix\n",
    "import pandas as pd\n",
    "import glob\n",
    "import re\n",
    "import io\n",
    "import numpy as np\n",
    "from collections import Counter\n",
    "import dask.dataframe as dd\n",
    "import numba as nb\n",
    "from glob import glob\n",
    "import pyximport;pyximport.install(language_level = '3',\n",
    "                                  setup_args={\"include_dirs\":np.get_include()})\n",
    "import cythongenomeanalyzer as cyGA\n",
    "import umap\n",
    "import time\n",
    "import hdbscan\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics.pairwise import pairwise_distances\n",
    "import math\n",
    "import numba as nb\n",
    "from numba.typed import Dict\n",
    "from numba import types\n",
    "import random\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "bb5b8adb-a0a8-4267-83f9-edb6f0c25bca",
   "metadata": {},
   "outputs": [],
   "source": [
    "##### basic numba functions\n",
    "\n",
    "@nb.njit(fastmath = True)\n",
    "def char2int(chars):\n",
    "    if chars == 65: return 0\n",
    "    elif chars == 67: return 1\n",
    "    elif chars == 71: return 2\n",
    "    elif chars == 84: return 3\n",
    "    return -1\n",
    "\n",
    "@nb.njit(fastmath = True)\n",
    "def invchar2int(chars):\n",
    "    if chars == 65: return 3\n",
    "    elif chars == 67: return 2\n",
    "    elif chars == 71: return 1\n",
    "    elif chars == 84: return 0\n",
    "    return -1\n",
    "\n",
    "@nb.njit(fastmath = True)\n",
    "def str2int(chars):\n",
    "    if chars == 'A' or chars == 'a': return 0\n",
    "    elif chars == 'C' or chars == 'c': return 1\n",
    "    elif chars == 'G' or chars == 'g': return 2\n",
    "    elif chars == 'T' or chars == 't': return 3\n",
    "    return -1\n",
    "\n",
    "@nb.njit(fastmath = True)\n",
    "def invstr2int(chars):\n",
    "    if chars == 'A' or chars == 'a': return 3\n",
    "    elif chars == 'C' or chars == 'c': return 2\n",
    "    elif chars == 'G' or chars == 'g': return 1\n",
    "    elif chars == 'T' or chars == 't': return 0\n",
    "    return -1\n",
    "\n",
    "\n",
    "@nb.njit(fastmath = True)\n",
    "def _numba_dict_fastq(file_mmap, klen, mg_fw_rv = True): \n",
    "    fw_div = 4**(klen-1)\n",
    "    is_line = 1 \n",
    "    window_pos = 0\n",
    "    fw_num = 0\n",
    "    rv_num = 0\n",
    "    result =  Dict.empty(key_type=types.uint64,value_type=types.uint32)\n",
    "    if mg_fw_rv == True:\n",
    "        for c in file_mmap:\n",
    "            if c == 78: window_pos, fw_num, rv_num = 0,0,0\n",
    "            elif c == 13: pass\n",
    "            elif c == 10: \n",
    "                is_line += 1\n",
    "                if is_line > 4: is_line= 1\n",
    "                window_pos = 0\n",
    "            elif is_line == 2: \n",
    "                fw_num = fw_num*4 + char2int(c)\n",
    "                rv_num = invchar2int(c)*(4**window_pos) + rv_num\n",
    "                window_pos += 1\n",
    "                if window_pos == klen: \n",
    "                    if fw_num < rv_num: \n",
    "                        try: result[fw_num] += types.uint32(1)\n",
    "                        except: result[fw_num] = types.uint32(1)\n",
    "                    else: \n",
    "                        try: result[rv_num] += types.uint32(1)\n",
    "                        except: result[fw_num] = types.uint32(1)\n",
    "                    fw_num %= fw_div\n",
    "                    rv_num //= 4\n",
    "                    window_pos -= 1\n",
    "    else:\n",
    "        for c in file_mmap:\n",
    "            if c == 78: window_pos, fw_num = 0,0\n",
    "            elif c == 13: pass\n",
    "            elif c == 10: \n",
    "                is_line += 1\n",
    "                if is_line > 4: is_line= 1\n",
    "                window_pos = 0\n",
    "            elif is_line == 2: \n",
    "                fw_num = fw_num*4 + char2int(c)\n",
    "                window_pos += 1\n",
    "                if window_pos == klen: \n",
    "                    try: result[fw_num] += types.int32(1)\n",
    "                    except: result[fw_num] = types.int32(1)\n",
    "                    \n",
    "                    fw_num %= fw_div\n",
    "                    window_pos -= 1\n",
    "    return result\n",
    "\n",
    "@nb.njit(fastmath = True)\n",
    "def _numba_dict_fasta(file_mmap, klen, mg_fw_rv = True): \n",
    "    fw_div = 4**(klen-1)\n",
    "    window_pos = 0\n",
    "    fw_num = 0\n",
    "    rv_num = 0\n",
    "    result =  Dict.empty(key_type=types.int64,value_type=types.uint32)\n",
    "    header_end = 0\n",
    "    if file_mmap[0] == 62: \n",
    "        while file_mmap[header_end] != 10: header_end+=1 \n",
    "            \n",
    "    if mg_fw_rv == True:\n",
    "        for c in file_mmap[header_end:]:\n",
    "            if c == 78: window_pos, fw_num, rv_num = 0,0,0\n",
    "            elif c == 10 or c == 13: pass\n",
    "            else: \n",
    "                fw_num = fw_num*4 + char2int(c)\n",
    "                rv_num = invchar2int(c)*(4**window_pos) + rv_num\n",
    "                window_pos += 1\n",
    "                if window_pos == klen: \n",
    "                    if fw_num < rv_num: \n",
    "                        try: result[fw_num] += types.uint32(1)\n",
    "                        except: result[fw_num] = types.uint32(1)\n",
    "                    else: \n",
    "                        try: result[rv_num] += types.uint32(1)\n",
    "                        except: result[fw_num] = types.uint32(1)\n",
    "                    fw_num %= fw_div\n",
    "                    rv_num //= 4\n",
    "                    window_pos -= 1\n",
    "    else:\n",
    "        for c in file_mmap[header_end:]:\n",
    "            if c == 78: window_pos, fw_num = 0,0\n",
    "            elif c == 10 or c == 13: pass\n",
    "            else: \n",
    "                fw_num = fw_num*4 + char2int(c)\n",
    "                window_pos += 1\n",
    "                if window_pos == klen: \n",
    "                    try: result[fw_num] += types.uint32(1)\n",
    "                    except: result[fw_num] = types.uint32(1)\n",
    "                    fw_num %= fw_div\n",
    "                    window_pos -= 1\n",
    "    return result\n",
    "\n",
    "@nb.njit(fastmath = True)\n",
    "def _numba_dict_string(string, klen,  mg_fw_rv = True): \n",
    "    fw_div = 4**(klen-1)\n",
    "    window_pos = 0\n",
    "    fw_num = 0\n",
    "    rv_num = 0\n",
    "    result =  Dict.empty(key_type=types.uint64,value_type=types.uint32)\n",
    "    if mg_fw_rv:\n",
    "        for c in string:\n",
    "            if c == 'N': window_pos, fw_num, rv_num = 0,0,0\n",
    "            elif c == '\\n'  or c == '\\r': pass\n",
    "            else: \n",
    "                fw_num = fw_num*4 + str2int(c)\n",
    "                rv_num = invstr2int(c)*(4**window_pos) + rv_num\n",
    "                window_pos += 1\n",
    "                if window_pos == klen: \n",
    "                    if fw_num < rv_num:\n",
    "                        try: result[fw_num] += types.uint32(1)\n",
    "                        except: result[fw_num] = types.uint32(1)\n",
    "                    else: \n",
    "                        try: result[rv_num] += types.uint32(1)\n",
    "                        except: result[fw_num] = types.uint32(1)\n",
    "                    fw_num %= fw_div\n",
    "                    rv_num //= 4\n",
    "                    window_pos -= 1\n",
    "    else:\n",
    "        for c in string:\n",
    "            if c == 'N': window_pos, fw_num = 0,0\n",
    "            elif c == '\\n' or c == '\\r': pass\n",
    "            else: \n",
    "                fw_num = fw_num*4 + str2int(c)\n",
    "                window_pos += 1\n",
    "                if window_pos == klen: \n",
    "                    try: result[fw_num] += types.uint32(1)\n",
    "                    except: result[fw_num] = types.uint32(1)\n",
    "                    fw_num %= fw_div\n",
    "                    window_pos -= 1\n",
    "    return result\n",
    "\n",
    "def dic2sparserow(dic, maxsize =  int(1e8)):\n",
    "    return sps.csr_matrix((list(dic.values()), list(dic.keys()), [0, len(dic)]), shape = (1, maxsize ))\n",
    "\n",
    "\n",
    "#@nb.jit\n",
    "def nb_multiple_files(filenames = [], klen = 10, cnt = 'auto', merge_reverse_complement = True):\n",
    "    max_size = 4**klen - 4**(klen//2) if merge_reverse_complement else 4**klen \n",
    "    if cnt == 'auto':\n",
    "        typ = filenames[0].split('.')[-1]\n",
    "        if typ == filenames[0]: typ = 'string'\n",
    "    else: typ = cnt    \n",
    "    if typ == 'fastq': \n",
    "        ret = sps.vstack((dic2sparserow(_numba_dict_fastq(np.memmap(name,  mode='r'), klen, merge_reverse_complement), max_size) for  name  in filenames))\n",
    "    elif typ == 'fasta' or typ == 'fa':\n",
    "        ret = sps.vstack((dic2sparserow(_numba_dict_fasta(name, klen, merge_reverse_complement), max_size) for  name  in filenames))\n",
    "    elif typ == 'string':\n",
    "        ret = sps.vstack((dic2sparserow(_numba_dict_string(np.memmap(name,  mode='r'), klen, merge_reverse_complement), max_size) for  name  in filenames))\n",
    "    else:\n",
    "        ret = sps.eye(0, dtype = np.uint32 ,format=\"csr\")\n",
    "    ret._meta = sps.eye(0, dtype = np.uint32 ,format=\"csr\")\n",
    "    return ret\n",
    "\n",
    "def nb_sparse_fasta(filenames = [], klen = 10, merge_reverse_complement = True):\n",
    "    max_size = 4**klen - 4**(klen//2) if merge_reverse_complement else 4**klen \n",
    "    ret = sps.vstack((dic2sparserow(_numba_dict_fasta(np.memmap(name,  mode='r'), klen, merge_reverse_complement), max_size) for  name  in filenames))\n",
    "    ret._meta = sps.eye(0, dtype = np.uint32 ,format=\"csr\")\n",
    "    return ret\n",
    "\n",
    "def nb_sparse_string(filenames = [], klen = 10,  merge_reverse_complement = True):\n",
    "    max_size = 4**klen - 4**(klen//2) if merge_reverse_complement else 4**klen \n",
    "    ret = sps.vstack((dic2sparserow(_numba_dict_string(name, klen, merge_reverse_complement), max_size) for  name  in filenames))\n",
    "    ret._meta = sps.eye(0, dtype = np.uint32 ,format=\"csr\")\n",
    "    return ret\n",
    "\n",
    "def nb_sparse_fastq(filenames = [], klen = 10, merge_reverse_complement = True):\n",
    "    max_size = 4**klen - 4**(klen//2) if merge_reverse_complement else 4**klen \n",
    "    ret = sps.vstack((dic2sparserow(_numba_dict_fastq(np.memmap(name,  mode='r'), klen, merge_reverse_complement), max_size) for  name  in filenames))\n",
    "    ret._meta = sps.eye(0, dtype = np.uint32 ,format=\"csr\")\n",
    "    return ret"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fccc50e1-461f-45d0-8bca-dfa2d53016ae",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "115151fc-cb04-40a6-946b-c6e56f8f7689",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 6.26 s\n"
     ]
    }
   ],
   "source": [
    "%time aaaa2 = nb_sparse_string([''.join(random.choices('ATCG', k=200)) for i in range(10000)], 30, False)\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4299cdeb-3088-4f61-bd12-33555a777639",
   "metadata": {},
   "outputs": [],
   "source": [
    "class KmerGenomeAnalyser:\n",
    "    \n",
    "    def __init__(self,minorallelefrequency = 0, kmer_size = 10, MergeReverseComplement = False):\n",
    "        if minorallelefrequency:\n",
    "            if type(minorallelefrequency) == int or type(minorallelefrequency) == float:\n",
    "                self.maf = sorted([minorallelefrequency, 1-minorallelefrequency])\n",
    "            elif type(minorallelefrequency) == list or type(minorallelefrequency) == tuple:\n",
    "                self.maf = sorted([minorallelefrequency[0], minorallelefrequency[1]])\n",
    "            else:\n",
    "                raise ValueError('Minor Allele Fequency not accepted')\n",
    "        else: self.maf =0\n",
    "        \n",
    "        if kmer_size < 0:\n",
    "            raise ValueError('Kmer size has to be positive')\n",
    "        else: self.kmer = kmer_size\n",
    "        \n",
    "        self.revComp = MergeReverseComplement\n",
    "        self.n_docs = 0\n",
    "                \n",
    "    def nbG2S(self, files = [], tfidf = False, binary_output = False, content = 'auto', single_row_output = False):\n",
    "        ncols = 4**self.kmer - 4**(self.kmer//2) if self.revComp else 4**self.kmer\n",
    "        if content == 'auto':\n",
    "            if (typ := files[0].split('.')[-1]) == files[0]: typ = 'string'\n",
    "        else: typ = content\n",
    "        \n",
    "        b = db.from_sequence(files)\n",
    "        if typ == 'cython':\n",
    "            #raise ValueError('Windows and Cython are not friends')\n",
    "            bag2 = b.map_partitions(cyGA.cyfiles2sparse, klen = self.kmer , merge_reverse_complement = self.revComp )\n",
    "        elif typ == 'fastq':\n",
    "            bag2 = b.map_partitions(self.nb_sparse_fastq, klen = self.kmer , merge_reverse_complement = self.revComp )\n",
    "        elif typ == 'fasta' or typ == 'fa':\n",
    "            bag2 = b.map_partitions(self.nb_sparse_fasta, klen = self.kmer , merge_reverse_complement = self.revComp )\n",
    "        elif typ == 'string':\n",
    "            bag2 = b.map_partitions(self.nb_sparse_string, klen = self.kmer , merge_reverse_complement = self.revComp )\n",
    "        else: return 'content type not found'\n",
    "\n",
    "        objs = bag2.to_delayed()\n",
    "        arrs = [da.from_delayed(obj, (np.nan, ncols), str) for obj in objs]\n",
    "        ret = da.concatenate(arrs, axis=0).compute()\n",
    "        \n",
    "        self.n_docs = ret.shape[0]\n",
    "                \n",
    "        if self.maf:\n",
    "            self.n_docs = ret.shape[0]\n",
    "            idx = np.unique(ret.indices, return_counts=True)\n",
    "            self.acceptedindices  =  idx[0][(self.maf[1]*self.n_docs >= idx[1])&(idx[1]>= self.maf[0]*self.n_docs)]\n",
    "            if len(self.acceptedindices) == 0:  raise ValueError('No kmers present with minor allele frequency range sent')\n",
    "            ret =  ret.tolil()[:,  self.acceptedindices ].tocsr()\n",
    "        \n",
    "        else:\n",
    "            self.acceptedindices = np.unique(ret.indices)\n",
    "        \n",
    "        if tfidf: ret =  TfidfTransformer().fit_transform(ret)\n",
    "        if binary_output: ret.data.fill(1)\n",
    "        if single_row_output: \n",
    "            ret = sps.csr_matrix((ret.data, ret.indices, (0, ret.indptr[-1])), dtype = np.int16, copy=False, shape = (1, ncols))\n",
    "            self.n_docs = 1\n",
    "        del arrs, objs, bag2, b\n",
    "        return ret\n",
    "    \n",
    "    def applymaf( X, maf):\n",
    "        if type(maf) == int or type(maf) == float:mafed = sorted([maf, 1-maf])\n",
    "        elif type(maf) == list or type(maf) == tuple: mafed = sorted([maf[0], maf[1]])\n",
    "        else: raise ValueError('Minor Allele Fequency not accepted')\n",
    "        warnings.warn(\"kmer IDs will be lost if <class>.maf is not altered too\") \n",
    "        n_doc = X.shape[0]\n",
    "        idx = np.unique(X.indices, return_counts=True)\n",
    "        out = idx[0][(mafed[1]*n_doc >= idx[1])&(idx[1]>= mafed[0]*n_doc)]\n",
    "        return X.tolil()[:, out].tocsr()\n",
    "\n",
    "    def dic2sparserow(dic, maxsize =  int(1e8)):\n",
    "        return sps.csr_matrix((list(dic.values()), list(dic.keys()), [0, len(dic)]), shape = (1, maxsize ))        \n",
    "\n",
    "    def kmer_list(self, matrix = None):\n",
    "        if matrix != None:\n",
    "            return [self.int2kmer(i) for i in self.acceptedindices]\n",
    "        else:     \n",
    "            return [self.int2kmer(i) for i in self.acceptedindices]\n",
    "        \n",
    "    \n",
    "    def int2kmer(self, i):\n",
    "        return np.base_repr(i,4).replace('0', 'A').replace('1', 'C').replace('2', 'G').replace('3', 'T')\n",
    "    \n",
    "    @staticmethod\n",
    "    @nb.njit(fastmath = True)\n",
    "    def str_to_int(s):\n",
    "        final_index, result = len(s) - 1, 0\n",
    "        for i,v in enumerate(s):\n",
    "            result += (ord(v) - 48) * (4 ** (final_index - i))\n",
    "        return result\n",
    "\n",
    "    def nb_sparse_fasta(self, filenames = [], klen = 10, merge_reverse_complement = True):\n",
    "        max_size = 4**klen - 4**(klen//2) if merge_reverse_complement else 4**klen \n",
    "        ret = sps.vstack((dic2sparserow(_numba_dict_fasta(np.memmap(name,  mode='r'), klen, merge_reverse_complement), max_size) for  name  in filenames))\n",
    "        ret._meta = sps.eye(0, dtype = np.uint32 ,format=\"csr\")\n",
    "        return ret\n",
    "\n",
    "    def nb_sparse_string(self,filenames = [], klen = 10,  merge_reverse_complement = True):\n",
    "        max_size = 4**klen - 4**(klen//2) if merge_reverse_complement else 4**klen \n",
    "        ret = sps.vstack((dic2sparserow(_numba_dict_string(name, klen, merge_reverse_complement), max_size) for  name  in filenames))\n",
    "        ret._meta = sps.eye(0, dtype = np.uint32 ,format=\"csr\")\n",
    "        return ret\n",
    "\n",
    "    def nb_sparse_fastq(self, filenames = [], klen = 10, merge_reverse_complement = True):\n",
    "        max_size = 4**klen - 4**(klen//2) if merge_reverse_complement else 4**klen \n",
    "        ret = sps.vstack((dic2sparserow(_numba_dict_fastq(np.memmap(name,  mode='r'), klen, merge_reverse_complement), max_size) for  name  in filenames))\n",
    "        ret._meta = sps.eye(0, dtype = np.uint32 ,format=\"csr\")\n",
    "        return ret\n",
    "    \n",
    "    def tfidf(X):\n",
    "        return TfidfTransformer().fit_transform(X)\n",
    "\n",
    "    def hellinger_dist_matrix(X):\n",
    "        return pairwise_distances(np.sqrt(X), n_jobs=-1, metric='euclidean') / math.sqrt(2)\n",
    "    \n",
    "    @staticmethod\n",
    "    @nb.njit(fastmath = True)\n",
    "    def str_to_int(s):\n",
    "        final_index, result = len(s) - 1, 0\n",
    "        for i,v in enumerate(s):\n",
    "            result += (ord(v) - 48) * (4 ** (final_index - i))\n",
    "        return result\n",
    "    \n",
    "    def plot_clusters(data, algorithm, args, kwds):\n",
    "        labels = algorithm(*args, **kwds).fit_predict(data)\n",
    "        palette = sns.color_palette('deep', np.unique(labels).max() + 1)\n",
    "        colors = [palette[x] if x >= 0 else (0.0, 0.0, 0.0) for x in labels]\n",
    "        plt.scatter(data.T[0], data.T[1], c=colors, **{'alpha' : 0.25, 's' : 80, 'linewidths':0})\n",
    "        frame = plt.gca()\n",
    "        frame.axes.get_xaxis().set_visible(False)\n",
    "        frame.axes.get_yaxis().set_visible(False)\n",
    "        sns.despine()\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3cf039c3-2ce8-454c-aac8-c40e9bfda67f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table style=\"border: 2px solid white;\">\n",
       "<tr>\n",
       "<td style=\"vertical-align: top; border: 0px solid white\">\n",
       "<h3 style=\"text-align: left;\">Client</h3>\n",
       "<ul style=\"text-align: left; list-style: none; margin: 0; padding: 0;\">\n",
       "  <li><b>Scheduler: </b>inproc://192.168.3.87/14388/1</li>\n",
       "  <li><b>Dashboard: </b><a href='http://192.168.3.87:8787/status' target='_blank'>http://192.168.3.87:8787/status</a></li>\n",
       "</ul>\n",
       "</td>\n",
       "<td style=\"vertical-align: top; border: 0px solid white\">\n",
       "<h3 style=\"text-align: left;\">Cluster</h3>\n",
       "<ul style=\"text-align: left; list-style:none; margin: 0; padding: 0;\">\n",
       "  <li><b>Workers: </b>1</li>\n",
       "  <li><b>Cores: </b>12</li>\n",
       "  <li><b>Memory: </b>16.91 GB</li>\n",
       "</ul>\n",
       "</td>\n",
       "</tr>\n",
       "</table>"
      ],
      "text/plain": [
       "<Client: 'inproc://192.168.3.87/14388/1' processes=1 threads=12, memory=16.91 GB>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from dask.distributed import Client\n",
    "\n",
    "client = Client(processes = False)\n",
    "client"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4a96e50d-f2d2-4a96-8432-a056b39216e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(sorted(glob('covid_fasta/*.fasta')) , columns = ['filename'])\n",
    "df['id'] = df.filename.apply(lambda x: x.strip('covid_fasta\\\\').strip('.fasta')[:-2]  )\n",
    "df2 = pd.read_csv('covid_metadata_w_us.csv')\n",
    "df3 = df.merge(df2, left_on= 'id', right_on='Accession')\n",
    "df4 = df3.sample(frac=1)\n",
    "df4.Collection_Date = df4.Collection_Date.apply(pd.to_datetime)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "252d5dd4-f391-405a-8dc4-f2dfcbb4c5c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "Analyser_class = KmerGenomeAnalyser(minorallelefrequency = 0, kmer_size = 20, MergeReverseComplement = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "fd21086f-f95b-43aa-b035-bc374eaa6028",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 2min 36s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<1000x1099511627776 sparse matrix of type '<class 'numpy.float64'>'\n",
       "\twith 28236673 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%time X = Analyser_class.nbG2S(df4[:1000].filename,  content ='fasta') # content = 'cython' int(1e8)\n",
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b8f15eb-cc41-437b-a388-d077b7f3e297",
   "metadata": {},
   "outputs": [],
   "source": [
    "%time Xsm = KmerGenomeAnalyser.applymaf(X, [.05,.95])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e102f7f1-5826-4b26-badf-6139b14f84ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "KmerGenomeAnalyser.tfidf(Xsm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17ee667a-b058-4e1f-acf4-30f91b991689",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "7ccde6d9-5817-4872-a1e4-caecf26ca68e",
   "metadata": {},
   "source": [
    "# regex tokenizer kmer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "id": "ae15be1c-c654-40de-ae93-0afeb2487c14",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1, 2, 3, 12]"
      ]
     },
     "execution_count": 122,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(set([1,2,3,1,12]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "id": "81cbe404-f0dc-4bea-b0a4-11e9ff1a37c9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 3, 4])"
      ]
     },
     "execution_count": 123,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.unique(np.array([1,3,4,]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a99a7f8-96f1-41c8-b927-397c5f061cec",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:UMAP]",
   "language": "python",
   "name": "conda-env-UMAP-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
